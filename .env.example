# ============================================================================
# DocIntel Environment Configuration
# ============================================================================
# Copy this file to .env and fill in your values
# Never commit .env to version control!

# ============================================================================
# API Keys (REQUIRED)
# ============================================================================

# OpenAI API Key - Get yours at: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-key-here

# Optional: Anthropic API Key (for future Claude integration)
# ANTHROPIC_API_KEY=your-key-here

# ============================================================================
# Model Configuration
# ============================================================================

# Embedding model for semantic search
# Options: text-embedding-3-small, text-embedding-3-large, text-embedding-ada-002
# Recommended: text-embedding-3-small (best cost/performance)
EMBEDDING_MODEL=text-embedding-3-small

# LLM model for response generation
# Options: gpt-4o-mini, gpt-4o, gpt-4-turbo, gpt-3.5-turbo
# Recommended: gpt-4o-mini (fast and cheap)
LLM_MODEL=gpt-4o-mini

# ============================================================================
# Chunking Configuration
# ============================================================================

# Size of each text chunk in tokens
# Larger = more context per chunk but less precise retrieval
# Smaller = more precise but may lose context
# Recommended: 512 (good balance)
CHUNK_SIZE=512

# Overlap between consecutive chunks in tokens
# Prevents information loss at chunk boundaries
# Recommended: 50 (about 10-15 words)
CHUNK_OVERLAP=50

# ============================================================================
# Retrieval Configuration
# ============================================================================

# Number of chunks to retrieve for each query
# More chunks = more context but slower and more expensive
# Recommended: 5 (covers ~2500 tokens)
TOP_K_RESULTS=5

# Minimum similarity threshold for retrieval (0-1)
# Higher = only very relevant chunks
# Lower = more chunks but potentially less relevant
# Recommended: 0.7
SIMILARITY_THRESHOLD=0.7

# ============================================================================
# File Storage Paths
# ============================================================================

# Directory for uploaded documents
UPLOAD_DIR=data/uploads

# Directory for vector database storage
VECTORDB_DIR=data/vectordb

# ============================================================================
# API Configuration (Optional)
# ============================================================================

# API host
# API_HOST=0.0.0.0

# API port
# API_PORT=8000

# Enable API debug mode (development only!)
# API_DEBUG=false

# ============================================================================
# Logging Configuration (Optional)
# ============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
# LOG_LEVEL=INFO

# Log file path
# LOG_FILE=logs/docintel.log

# ============================================================================
# Advanced Settings (Optional)
# ============================================================================

# Temperature for LLM (0-1, lower = more deterministic)
# LLM_TEMPERATURE=0.1

# Maximum tokens for LLM response
# LLM_MAX_TOKENS=1000

# Batch size for embedding generation
# EMBEDDING_BATCH_SIZE=100

# ============================================================================
# Notes
# ============================================================================
#
# Cost Estimates (approximate):
# - text-embedding-3-small: $0.02 per 1M tokens
# - gpt-4o-mini: $0.15 per 1M input tokens, $0.60 per 1M output tokens
# - Average query: ~$0.0001 (800 input + 200 output tokens)
# - 100 queries â‰ˆ $0.01
#
# Performance Tips:
# - Use text-embedding-3-small for best cost/performance
# - Use gpt-4o-mini for fast, cheap responses
# - Increase CHUNK_SIZE for more context (but slower)
# - Increase TOP_K_RESULTS for better accuracy (but more expensive)
#
# Production Recommendations:
# - Never commit .env to git (use .gitignore)
# - Use environment-specific .env files (.env.dev, .env.prod)
# - Store secrets in secure vaults (AWS Secrets Manager, Azure Key Vault)
# - Implement API key rotation
# - Monitor API usage and costs
#
# ============================================================================