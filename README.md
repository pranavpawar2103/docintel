<div align="center">

# ğŸ“š DocIntel

### Intelligent Document Analysis with Retrieval-Augmented Generation

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Tests](https://img.shields.io/badge/tests-42%2F46%20passing-brightgreen.svg)](tests/)

[Features](#-features) â€¢
[Demo](#-demo) â€¢
[Installation](#-installation) â€¢
[Usage](#-usage) â€¢
[Architecture](#-architecture) â€¢
[Performance](#-performance) â€¢
[Documentation](#-documentation)

</div>

---

## ğŸ¯ Overview

DocIntel is a production-ready **Retrieval-Augmented Generation (RAG)** system that enables natural language querying of documents. Upload PDFs, Word documents, or text files, and ask questions in plain English to get accurate answers with source citations.

### Why DocIntel?

- **ğŸ¯ High Accuracy**: 85%+ answer accuracy with automatic source citations
- **âš¡ Fast**: Sub-3-second query latency with intelligent caching
- **ğŸ’° Cost-Effective**: ~$0.01 per 100 queries using optimized API calls
- **ğŸ”’ Production-Ready**: Comprehensive error handling, logging, and monitoring
- **ğŸ“ˆ Scalable**: Built to handle thousands of documents and concurrent users

---

## âœ¨ Features

### Core Capabilities

- **Multi-Format Support**: PDF, DOCX, TXT, Markdown
- **Intelligent Chunking**: Semantic-aware text splitting with contextual overlap
- **Semantic Search**: Vector similarity search using OpenAI embeddings
- **Source Citations**: Automatic citation tracking with page numbers
- **Confidence Scoring**: Transparent confidence metrics for each answer
- **Conversation Memory**: Multi-turn conversations with context retention

### Technical Features

- **Modern Stack**: FastAPI + Streamlit + ChromaDB + OpenAI
- **RESTful API**: Complete API with automatic documentation (Swagger/OpenAPI)
- **Production-Grade**: Comprehensive testing, logging, error handling
- **Optimized Performance**: Batched API calls, efficient vector search
- **Developer-Friendly**: Well-documented code with type hints

---

## ğŸ¬ Demo

### Quick Start Example
```python
from src.retrieval.rag_pipeline import RAGPipeline

# Initialize system
rag = RAGPipeline()

# Ask a question
response = rag.query("What is machine learning?")

print(response['answer'])
print(f"Confidence: {response['confidence']:.2%}")
print(f"Sources: {len(response['sources'])}")
```

### Web Interface

![DocIntel Interface](docs/images/interface-screenshot.png)

*Upload documents, ask questions, and get instant answers with citations*

---

## ğŸš€ Installation

### Prerequisites

- Python 3.11 or higher
- OpenAI API key ([Get one here](https://platform.openai.com/api-keys))

### Quick Install
```bash
# Clone the repository
git clone https://github.com/yourusername/docintel.git
cd docintel

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -e .

# Set up environment variables
cp .env.example .env
# Edit .env and add your OPENAI_API_KEY
```

### Configuration

Create a `.env` file in the project root:
```env
OPENAI_API_KEY=your_api_key_here
EMBEDDING_MODEL=text-embedding-3-small
LLM_MODEL=gpt-4o-mini
CHUNK_SIZE=512
CHUNK_OVERLAP=50
```

---

## ğŸ’» Usage

### Option 1: Web Interface (Recommended)
```bash
# Terminal 1: Start Backend API
uvicorn src.api.main:app --reload --port 8000

# Terminal 2: Start Frontend
streamlit run streamlit_app/app.py
```

Then open your browser to `http://localhost:8501`

### Option 2: Python API
```python
from src.ingestion.pipeline import IngestionPipeline
from src.retrieval.rag_pipeline import RAGPipeline

# Initialize pipelines
ingestion = IngestionPipeline()
rag = RAGPipeline(vector_store=ingestion.vector_store)

# Ingest documents
result = ingestion.ingest_document("path/to/document.pdf")
print(f"Processed {result['num_chunks']} chunks")

# Query the system
response = rag.query("What is the main topic?")
print(response['answer'])

# View sources
for source in response['sources']:
    print(f"- {source['document_name']}, Page {source['page_number']}")
```

### Option 3: REST API
```bash
# Upload document
curl -X POST "http://localhost:8000/api/documents/upload" \
  -F "file=@document.pdf"

# Query
curl -X POST "http://localhost:8000/api/query" \
  -H "Content-Type: application/json" \
  -d '{"question": "What is RAG?"}'

# API Documentation: http://localhost:8000/docs
```

---

## ğŸ—ï¸ Architecture

### System Overview
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     USER INTERFACE                      â”‚
â”‚              Streamlit Web App / REST API               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   RAG PIPELINE                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Document   â”‚  â”‚   Retrieval  â”‚  â”‚  Generation  â”‚   â”‚
â”‚  â”‚  Ingestion   â”‚â”€â–¶   (Vector    |â”€â–¶â”‚     (LLM)    â”‚   |
â”‚  â”‚              â”‚  â”‚    Search)   â”‚  â”‚              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DATA LAYER                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   ChromaDB   â”‚  â”‚    OpenAI    â”‚  â”‚   Document   â”‚   â”‚
â”‚  â”‚  (Vectors)   â”‚  â”‚ (Embeddings) â”‚  â”‚   Storage    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### RAG Pipeline Flow
```
1. INGESTION
   â”œâ”€â”€ Parse Document (PDF/DOCX/TXT)
   â”œâ”€â”€ Intelligent Chunking (512 tokens, 50 overlap)
   â”œâ”€â”€ Generate Embeddings (OpenAI text-embedding-3-small)
   â””â”€â”€ Store in Vector DB (ChromaDB)

2. RETRIEVAL
   â”œâ”€â”€ Convert Query to Embedding
   â”œâ”€â”€ Similarity Search (Cosine Distance)
   â””â”€â”€ Retrieve Top-K Chunks (default: 5)

3. GENERATION
   â”œâ”€â”€ Build Prompt with Context
   â”œâ”€â”€ Generate Response (GPT-4o-mini)
   â”œâ”€â”€ Extract Citations
   â””â”€â”€ Calculate Confidence Score
```

---

## ğŸ“Š Performance

### Key Metrics

| Metric | Value |
|--------|-------|
| **Query Latency** | 2-3 seconds (average) |
| **Ingestion Speed** | ~50 chunks/second |
| **Answer Accuracy** | 85%+ on test queries |
| **Cost per Query** | ~$0.0001 |
| **Test Coverage** | 91% (42/46 tests passing) |

### Detailed Performance Report

See [PERFORMANCE.md](PERFORMANCE.md) for comprehensive benchmarks and optimization recommendations.

---

## ğŸ“ Project Structure
```
docintel/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/          # Document processing
â”‚   â”‚   â”œâ”€â”€ parser.py       # Multi-format document parsing
â”‚   â”‚   â”œâ”€â”€ chunker.py      # Intelligent text chunking
â”‚   â”‚   â””â”€â”€ pipeline.py     # Ingestion orchestration
â”‚   â”‚
â”‚   â”œâ”€â”€ retrieval/          # RAG components
â”‚   â”‚   â”œâ”€â”€ embeddings.py   # OpenAI embedding generation
â”‚   â”‚   â”œâ”€â”€ vectorstore.py  # ChromaDB interface
â”‚   â”‚   â”œâ”€â”€ llm.py          # LLM response generation
â”‚   â”‚   â””â”€â”€ rag_pipeline.py # Complete RAG workflow
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                # FastAPI backend
â”‚   â”‚   â”œâ”€â”€ main.py         # API endpoints
â”‚   â”‚   â””â”€â”€ models.py       # Pydantic models
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ config.py       # Configuration management
â”‚
â”œâ”€â”€ streamlit_app/          # Web interface
â”‚   â””â”€â”€ app.py              # Streamlit application
â”‚
â”œâ”€â”€ tests/                  # Comprehensive test suite
â”‚   â”œâ”€â”€ test_parser.py
â”‚   â”œâ”€â”€ test_chunker.py
â”‚   â”œâ”€â”€ test_embeddings.py
â”‚   â”œâ”€â”€ test_vectorstore.py
â”‚   â”œâ”€â”€ test_rag_pipeline.py
â”‚   â””â”€â”€ test_integration.py
â”‚
â”œâ”€â”€ scripts/                # Utility scripts
â”‚   â”œâ”€â”€ benchmark_system.py
â”‚   â””â”€â”€ generate_performance_report.py
â”‚
â”œâ”€â”€ docs/                   # Documentation
â”‚   â”œâ”€â”€ API.md              # API documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md     # Architecture details
â”‚   â””â”€â”€ CONTRIBUTING.md     # Contribution guidelines
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ uploads/            # Uploaded documents
â”‚   â””â”€â”€ vectordb/           # ChromaDB storage
â”‚
â”œâ”€â”€ .env                    # Environment variables (not in git)
â”œâ”€â”€ .env.example            # Example configuration
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ setup.py                # Package setup
â”œâ”€â”€ pytest.ini              # Test configuration
â”œâ”€â”€ README.md               # This file
â””â”€â”€ PERFORMANCE.md          # Performance benchmarks
```

---

## ğŸ§ª Testing

### Run Tests
```bash
# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=src --cov-report=html

# Run specific test file
pytest tests/test_rag_pipeline.py -v

# Run integration tests only
pytest tests/ -v -m integration
```

### Performance Benchmarks
```bash
# Run benchmarks
python scripts/benchmark_system.py

# Generate performance report
python scripts/generate_performance_report.py
```

---

## ğŸ”§ Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `OPENAI_API_KEY` | OpenAI API key (required) | - |
| `EMBEDDING_MODEL` | Embedding model name | text-embedding-3-small |
| `LLM_MODEL` | LLM model name | gpt-4o-mini |
| `CHUNK_SIZE` | Token count per chunk | 512 |
| `CHUNK_OVERLAP` | Overlap between chunks | 50 |
| `TOP_K_RESULTS` | Number of chunks to retrieve | 5 |

### Customization

See [docs/CONFIGURATION.md](docs/CONFIGURATION.md) for advanced configuration options.

---

## ğŸ“š Documentation

- **[API Documentation](docs/API.md)**: Complete REST API reference
- **[Architecture Guide](docs/ARCHITECTURE.md)**: Detailed system architecture
- **[Contributing Guide](docs/CONTRIBUTING.md)**: How to contribute
- **[Performance Report](PERFORMANCE.md)**: Benchmarks and optimization
- **[Demo Video Script](docs/DEMO_SCRIPT.md)**: Video walkthrough guide

---

## ğŸ›£ï¸ Roadmap

### Current Status (v1.0)
- âœ… Core RAG functionality
- âœ… Multi-format document support
- âœ… Web interface and REST API
- âœ… Comprehensive testing

### Planned Features (v1.1+)
- [ ] Multi-user support with authentication
- [ ] Document comparison and analysis
- [ ] Advanced filtering (by date, author, tags)
- [ ] Export Q&A history
- [ ] Conversation context management
- [ ] Support for more document formats
- [ ] Cloud deployment (Docker, AWS, GCP)
- [ ] Integration with Pinecone/Weaviate
- [ ] Real-time collaboration features

---

## ğŸ¤ Contributing

Contributions are welcome! Please read [CONTRIBUTING.md](docs/CONTRIBUTING.md) for details on our code of conduct and the process for submitting pull requests.

### Development Setup
```bash
# Install dev dependencies
pip install -e ".[dev]"

# Run tests
pytest tests/ -v

# Format code
black src/ tests/ streamlit_app/

# Lint code
flake8 src/ tests/ streamlit_app/
```

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](docs/CONTRIBUTING.md) file for details.

---

## ğŸ™ Acknowledgments

- **OpenAI** for GPT-4o-mini and embedding models
- **ChromaDB** for vector database
- **LangChain** for RAG framework components
- **FastAPI** for modern API framework
- **Streamlit** for rapid UI development

---

## ğŸ“§ Contact

**Pranav** - Master's in Computer Science, University of Ottawa

- LinkedIn: [linkedin.com/in/PranavPawar](https://www.linkedin.com/in/pranav-pawar-4175741b3/)
- Email: pranavpawar2126@gmail.com
---

## ğŸŒŸ Show Your Support

Give a â­ï¸ if this project helped you!

---

<div align="center">

**Built with â¤ï¸ using Python, FastAPI, Streamlit, and OpenAI**

[â¬† back to top](#-docintel)

</div>